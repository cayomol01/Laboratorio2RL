{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es un Markov Decision Process (MDP)?\n",
    "    a. Una MDP se trata de un tipo de machine learning en el que se quiere aumentar el rendimiento de la máquina. Lo que hacen es tener una recompensa que se conoce como señal de refuerzo para que así el agente sea capaz de tomar decisiones en torno a un ambiente en específico o estocástico. \n",
    "\n",
    "2. ¿Cuáles son los componentes principales de un MDP?\n",
    "    a. Los componentes principales de un MDP son:\n",
    "        - Estados, acciones, función de transición, recompensa y política.\n",
    "\n",
    "3. ¿Cuál es el objetivo principal del aprendizaje por refuerzo con MDPs?\n",
    "    a. El objetivo principal se trata acerca de que el agente explore distintas situaciones y que en base al ambiente o entorno en el que se encuentre, haga que la recompensa sea siempre mayor y mejor optimizada.\n",
    "\n",
    "GeeksforGeeks. (2018, January 4). Markov Decision Process. GeeksforGeeks; GeeksforGeeks. https://www.geeksforgeeks.org/markov-decision-process/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP():\n",
    "    def __init__(self):\n",
    "        self.states = self.getStates()\n",
    "        self.actions = self.getActions()\n",
    "        self.map = self.getMap()\n",
    "        self.transitions = self.getTransitions()\n",
    "\n",
    "        self.rewards = self.getRewards()\n",
    "        self.policy = self.getPolicy()\n",
    "    \n",
    "    def simulatePolicy(self,steps, initialState=0):\n",
    "        rewardAcc = 0\n",
    "        estado = initialState\n",
    "        for _ in range(steps):\n",
    "            action = self.policy[estado]\n",
    "            sPrime = self.transitions[estado][action]\n",
    "            reward = self.rewards[estado][action][sPrime]\n",
    "            \n",
    "            rewardAcc += reward\n",
    "            estado = sPrime\n",
    "            \n",
    "            if estado == 2:  # Si el robot alcanza la meta (G), terminamos la simulación\n",
    "                break\n",
    "        return rewardAcc\n",
    "    \n",
    "    def getStates(self):\n",
    "        states = [i for i in range(9)]\n",
    "        return states\n",
    "\n",
    "    def getMapSymbol(self,pos):\n",
    "        return self.map[pos]\n",
    "\n",
    "    def getRewards(self):\n",
    "        rewards = {}\n",
    "        for i in self.states:\n",
    "            rewards[i] = {}\n",
    "            for j in self.actions:\n",
    "                sPrime = self.transitions[i][j]\n",
    "                if self.getMapSymbol(sPrime) == 'G':\n",
    "                    rewards[i][j] = {sPrime: 1}\n",
    "                elif self.getMapSymbol(sPrime) == 'X':\n",
    "                    rewards[i][j] = {sPrime: -1}\n",
    "                else:\n",
    "                    rewards[i][j] = {sPrime: 0}\n",
    "        return rewards\n",
    "                \n",
    "    def getPolicy(self):\n",
    "        policy = {}\n",
    "        for i in self.states:\n",
    "            if i <3:\n",
    "                policy[i] = 'derecha'\n",
    "            else:\n",
    "                policy[i] = 'arriba'\n",
    "        return policy\n",
    "    \n",
    "    def checkPosition(self, state, action):\n",
    "        indexState = self.states.index(state)\n",
    "        if action=='arriba':\n",
    "            if (indexState-3) < 0:\n",
    "                return indexState\n",
    "            else:\n",
    "                return indexState-3\n",
    "        elif action == 'abajo':\n",
    "            if (indexState+3) > len(self.states)-1:\n",
    "                return indexState\n",
    "            else:\n",
    "                return indexState+3\n",
    "        elif action=='izquierda':\n",
    "            if (indexState%3)== 0 :\n",
    "                return indexState\n",
    "            else:\n",
    "                return indexState-1\n",
    "        elif action=='derecha':\n",
    "            if (indexState%3)== 2:\n",
    "                return indexState\n",
    "            else:\n",
    "                return indexState+1\n",
    "        \n",
    "    def getTransitions(self):\n",
    "        p = {}\n",
    "        for i in self.states:\n",
    "            p[i] = {}\n",
    "            for j in self.actions:\n",
    "                p[i][j] = self.checkPosition(i, j)\n",
    "        return p\n",
    "    \n",
    "    def getMap(self):\n",
    "        map = ['S', ' ', 'G',\n",
    "               ' ', 'X', ' ',\n",
    "               ' ', ' ', 'X']\n",
    "        return map\n",
    "\n",
    "    def getActions(self):\n",
    "        actions = ['arriba', 'abajo', 'izquierda', 'derecha']\n",
    "        return actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePolicy(mdp, steps, simulations):\n",
    "    markovProcess = mdp\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(simulations):\n",
    "        reward = markovProcess.simulatePolicy(steps)\n",
    "        rewards.append(reward)\n",
    "    return sum(rewards)/simulations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'arriba': 0, 'abajo': 3, 'izquierda': 0, 'derecha': 1}\n",
      "1 {'arriba': 1, 'abajo': 4, 'izquierda': 0, 'derecha': 2}\n",
      "2 {'arriba': 2, 'abajo': 5, 'izquierda': 1, 'derecha': 2}\n",
      "3 {'arriba': 0, 'abajo': 6, 'izquierda': 3, 'derecha': 4}\n",
      "4 {'arriba': 1, 'abajo': 7, 'izquierda': 3, 'derecha': 5}\n",
      "5 {'arriba': 2, 'abajo': 8, 'izquierda': 4, 'derecha': 5}\n",
      "6 {'arriba': 3, 'abajo': 6, 'izquierda': 6, 'derecha': 7}\n",
      "7 {'arriba': 4, 'abajo': 7, 'izquierda': 6, 'derecha': 8}\n",
      "8 {'arriba': 5, 'abajo': 8, 'izquierda': 7, 'derecha': 8}\n",
      "\n",
      "{0: 'derecha', 1: 'derecha', 2: 'derecha', 3: 'arriba', 4: 'arriba', 5: 'arriba', 6: 'arriba', 7: 'arriba', 8: 'arriba'}\n",
      "\n",
      "0 {'arriba': {0: 0}, 'abajo': {3: 0}, 'izquierda': {0: 0}, 'derecha': {1: 0}}\n",
      "1 {'arriba': {1: 0}, 'abajo': {4: -1}, 'izquierda': {0: 0}, 'derecha': {2: 1}}\n",
      "2 {'arriba': {2: 1}, 'abajo': {5: 0}, 'izquierda': {1: 0}, 'derecha': {2: 1}}\n",
      "3 {'arriba': {0: 0}, 'abajo': {6: 0}, 'izquierda': {3: 0}, 'derecha': {4: -1}}\n",
      "4 {'arriba': {1: 0}, 'abajo': {7: 0}, 'izquierda': {3: 0}, 'derecha': {5: 0}}\n",
      "5 {'arriba': {2: 1}, 'abajo': {8: -1}, 'izquierda': {4: -1}, 'derecha': {5: 0}}\n",
      "6 {'arriba': {3: 0}, 'abajo': {6: 0}, 'izquierda': {6: 0}, 'derecha': {7: 0}}\n",
      "7 {'arriba': {4: -1}, 'abajo': {7: 0}, 'izquierda': {6: 0}, 'derecha': {8: -1}}\n",
      "8 {'arriba': {5: 0}, 'abajo': {8: -1}, 'izquierda': {7: 0}, 'derecha': {8: -1}}\n",
      "\n",
      "Reward:  1\n",
      "\n",
      "Average accumlated reward 1.0\n"
     ]
    }
   ],
   "source": [
    "markov = MDP()\n",
    "\n",
    "for key, value in markov.transitions.items():\n",
    "    print(key, value)\n",
    "    \n",
    "print()\n",
    "print(markov.policy)\n",
    "print()\n",
    "for key, value in markov.rewards.items():\n",
    "    print(key,value)\n",
    "\n",
    "print()\n",
    "print('Reward: ', markov.simulatePolicy(10))\n",
    "\n",
    "print()\n",
    "print('Average accumlated reward', evaluatePolicy(markov, 10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
